{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj95UPolgOdfuInysCoMcN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turna1/CISC801-Topics-in-AI/blob/main/W1_Lab1_CISC801.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1 Simple Classifier"
      ],
      "metadata": {
        "id": "j7be9oTjpBBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get our hands dirty with a little bit of code, let's write a simple classifier using the traditional, rule-based approach. This will show us some basic Python syntax and highlight why this method isn't powerful enough for complex problems. Our function will classify a number as \"small,\" \"medium,\" or \"large.\""
      ],
      "metadata": {
        "id": "vM16f_W1oqOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a function using the 'def' keyword.\n",
        "# This function takes one input, 'number'.\n",
        "def classify_number(number):\n",
        "    \"\"\"\n",
        "    This function takes a number and returns a string label:\n",
        "    \"small\", \"medium\", or \"large\" based on hand-written rules.\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing the number: {number}\")\n",
        "\n",
        "    # The 'if' statement checks a condition.\n",
        "    # If the number is less than 10, the indented code below runs.\n",
        "    if number < 10:\n",
        "        return \"small\"\n",
        "\n",
        "    # 'elif' is short for \"else if\". It checks another condition.\n",
        "    elif number < 100:\n",
        "        return \"medium\"\n",
        "\n",
        "    # 'else' runs if none of the above conditions were true.\n",
        "    else:\n",
        "        return \"large\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16wvoJJtotbz",
        "outputId": "32b46116-85b3-42a6-893c-82e6bc54d69b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing the number: 5\n",
            "The result is: small\n",
            "Analyzing the number: 50\n",
            "The result is: medium\n",
            "Analyzing the number: 500\n",
            "The result is: large\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's test our function.\n",
        "result1 = classify_number(5)\n",
        "print(f\"The result is: {result1}\")\n",
        "\n",
        "result2 = classify_number(50)\n",
        "print(f\"The result is: {result2}\")\n",
        "\n",
        "result3 = classify_number(500)\n",
        "print(f\"The result is: {result3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIv7o1O1pRm8",
        "outputId": "721118fd-94ff-4202-a383-82706959e597"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing the number: 5\n",
            "The result is: small\n",
            "Analyzing the number: 50\n",
            "The result is: medium\n",
            "Analyzing the number: 500\n",
            "The result is: large\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works perfectly for our simple problem because the rules are clear and unchanging. But imagine trying to write ‘if’ statements for the millions of pixel values in a picture of a cat—it would be impossible! This is why we need a model that can learn the rules on its own."
      ],
      "metadata": {
        "id": "EnbWGjnOpiaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Finding Patterns in a Simple List\n"
      ],
      "metadata": {
        "id": "GgECAFYXsWj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a simple Python script that mimics the goal of unsupervised learning. We won't use an ML library, but we'll write code that finds an inherent pattern in a list of words—in this case, we'll group them by their starting letter."
      ],
      "metadata": {
        "id": "h62pqLk0sT3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is our unlabeled list of words.\n",
        "words = [\"apple\", \"banana\", \"ant\", \"boat\", \"car\", \"cat\", \"anchor\"]\n",
        "\n",
        "# We will store our groups in a dictionary.\n",
        "# A dictionary stores key-value pairs, like {\"a\": [\"apple\", \"ant\"]}.\n",
        "grouped_words = {}\n",
        "\n",
        "# A 'for' loop lets us check every word in our list.\n",
        "for word in words:\n",
        "    # Get the first letter of the current word.\n",
        "    first_letter = word[0]\n",
        "\n",
        "    # Check if we have already started a group for this letter.\n",
        "    if first_letter not in grouped_words:\n",
        "        # If not, create a new empty list for this letter.\n",
        "        grouped_words[first_letter] = []\n",
        "\n",
        "    # Add the current word to the group for its first letter.\n",
        "    grouped_words[first_letter].append(word)\n",
        "\n"
      ],
      "metadata": {
        "id": "jjNEYEY2skvq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the organized groups.\n",
        "print(\"Found the following groups:\")\n",
        "print(grouped_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmsruSLVpaxp",
        "outputId": "8f9f6f54-777f-4baa-ebf3-b9b2d89e1c5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found the following groups:\n",
            "{'a': ['apple', 'ant', 'anchor'], 'b': ['banana', 'boat'], 'c': ['car', 'cat']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3:"
      ],
      "metadata": {
        "id": "5eBY4E-F0RTS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrJ66ZeU0ZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c873ed55"
      },
      "source": [
        "## Example 7: Reinforcement Learning - Simplified Cart-Pole Balancing\n",
        "\n",
        "The Cart-Pole problem is a classic Reinforcement Learning task. The goal is to balance a pole on a cart by moving the cart left or right. The agent receives a reward for every timestep the pole remains upright. If the pole falls beyond a certain angle or the cart moves too far off-center, the episode ends.\n",
        "\n",
        "For simplicity, we'll create a highly abstract version focusing on the core RL loop rather than a full physics simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cba7e2"
      },
      "source": [
        "## Example 3: Reinforcement Learning - Simplified Cart-Pole Balancing\n",
        "\n",
        "The Cart-Pole problem is a classic Reinforcement Learning task. The goal is to balance a pole on a cart by moving the cart left or right. The agent receives a reward for every timestep the pole remains upright. If the pole falls beyond a certain angle or the cart moves too far off-center, the episode ends.\n",
        "\n",
        "For simplicity, we'll create a highly abstract version focusing on the core RL loop rather than a full physics simulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e21d8c7",
        "outputId": "07e1e5d4-cedd-4669-9c8e-144c7c20e577"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Simplified Cart-Pole Environment ---\n",
        "# State: [pole_angle, cart_position]\n",
        "# Actions: 0 (move left), 1 (move right)\n",
        "\n",
        "def initialize_environment():\n",
        "    # Pole starts slightly off-center, cart at center\n",
        "    pole_angle = random.uniform(-0.05, 0.05) # radians, small random tilt\n",
        "    cart_position = 0.0 # meters\n",
        "    return [pole_angle, cart_position]\n",
        "\n",
        "def step(state, action):\n",
        "    pole_angle, cart_position = state\n",
        "\n",
        "    # Simulate simple physics (highly simplified)\n",
        "    # Moving cart impacts pole angle and position\n",
        "    if action == 0: # Move left\n",
        "        cart_position -= 0.1\n",
        "        pole_angle += 0.03 # Pole might tilt more if moving fast\n",
        "    else: # Move right\n",
        "        cart_position += 0.1\n",
        "        pole_angle -= 0.03\n",
        "\n",
        "    # Add some random disturbance to pole angle\n",
        "    pole_angle += random.uniform(-0.02, 0.02)\n",
        "\n",
        "    # Define termination conditions\n",
        "    done = False\n",
        "    reward = 1.0 # Reward for keeping the pole balanced\n",
        "\n",
        "    # Pole falls if angle is too extreme\n",
        "    if abs(pole_angle) > 0.2: # ~11 degrees\n",
        "        reward = 0.0 # No reward if pole falls\n",
        "        done = True\n",
        "\n",
        "    # Cart goes off track\n",
        "    if abs(cart_position) > 2.0: # meters\n",
        "        reward = 0.0\n",
        "        done = True\n",
        "\n",
        "    next_state = [pole_angle, cart_position]\n",
        "    return next_state, reward, done\n",
        "\n",
        "print(\"Simplified Cart-Pole environment setup.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified Cart-Pole environment setup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2554f28"
      },
      "source": [
        "### The Agent (Q-Learning with State Discretization)\n",
        "\n",
        "Since the state space (pole angle and cart position) is continuous, we need to **discretize** it into bins so we can build a Q-table. Our Q-learning agent will then learn the best action for each discretized state.\n",
        "\n",
        "*   **State discretization**: Divide the continuous ranges of pole angle and cart position into a fixed number of bins.\n",
        "*   **Q-table**: Stores `Q[angle_bin, position_bin, action]` values.\n",
        "*   **Epsilon-Greedy**: For balancing exploration and exploitation.\n",
        "*   **Learning Rate (alpha)** and **Discount Factor (gamma)**: As before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad349065",
        "outputId": "a87d6c4a-f23f-4a94-c954-6c8147ee5d72"
      },
      "source": [
        "# --- Q-Learning Agent Parameters ---\n",
        "# Discretization for pole_angle and cart_position\n",
        "angle_bins = np.linspace(-0.2, 0.2, 10) # 10 bins for pole angle\n",
        "position_bins = np.linspace(-2.0, 2.0, 10) # 10 bins for cart position\n",
        "\n",
        "def discretize_state(state):\n",
        "    angle, position = state\n",
        "    angle_idx = np.digitize(angle, angle_bins) - 1\n",
        "    position_idx = np.digitize(position, position_bins) - 1\n",
        "\n",
        "    # Handle out-of-bounds due to digitize behavior, clamp to valid indices\n",
        "    angle_idx = np.clip(angle_idx, 0, len(angle_bins) - 1)\n",
        "    position_idx = np.clip(position_idx, 0, len(position_bins) - 1)\n",
        "\n",
        "    return int(angle_idx), int(position_idx)\n",
        "\n",
        "# Q-table dimensions: (num_angle_bins, num_position_bins, num_actions)\n",
        "q_table = np.zeros((len(angle_bins), len(position_bins), 2))\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.2       # Exploration rate\n",
        "learning_rate = 0.5 # Alpha\n",
        "discount_factor = 0.9 # Gamma\n",
        "num_episodes = 5000  # Number of training episodes\n",
        "max_steps_per_episode = 200 # Max steps before an episode is forced to end\n",
        "\n",
        "print(\"Starting Q-learning training for Cart-Pole...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    current_state = initialize_environment()\n",
        "    total_episode_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps_per_episode:\n",
        "        angle_idx, position_idx = discretize_state(current_state)\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.uniform(0, 1) < epsilon: # Explore\n",
        "            action = random.randrange(2) # 0 or 1\n",
        "        else: # Exploit\n",
        "            action = np.argmax(q_table[angle_idx, position_idx, :])\n",
        "\n",
        "        next_state, reward, done = step(current_state, action)\n",
        "        next_angle_idx, next_position_idx = discretize_state(next_state)\n",
        "\n",
        "        # Q-learning update rule\n",
        "        old_q_value = q_table[angle_idx, position_idx, action]\n",
        "        next_max_q = np.max(q_table[next_angle_idx, next_position_idx, :])\n",
        "\n",
        "        new_q_value = old_q_value + learning_rate * (reward + discount_factor * next_max_q - old_q_value)\n",
        "        q_table[angle_idx, position_idx, action] = new_q_value\n",
        "\n",
        "        current_state = next_state\n",
        "        total_episode_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    epsilon = max(0.01, epsilon * 0.995)\n",
        "\n",
        "    if (episode + 1) % 500 == 0:\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_episode_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"\\n--- Cart-Pole Q-Learning Training Complete ---\")\n",
        "\n",
        "# --- Test the trained agent (simulate one episode) ---\n",
        "print(\"\\nTesting the trained agent for one episode...\")\n",
        "current_state = initialize_environment()\n",
        "total_test_reward = 0\n",
        "done = False\n",
        "test_steps = 0\n",
        "\n",
        "while not done and test_steps < max_steps_per_episode * 2: # Allow more steps for testing\n",
        "    angle_idx, position_idx = discretize_state(current_state)\n",
        "    action = np.argmax(q_table[angle_idx, position_idx, :]) # Always exploit in test\n",
        "    next_state, reward, done = step(current_state, action)\n",
        "    total_test_reward += reward\n",
        "    current_state = next_state\n",
        "    test_steps += 1\n",
        "\n",
        "print(f\"Test Episode finished in {test_steps} steps with total reward: {total_test_reward}\")\n",
        "if total_test_reward > 0:\n",
        "    print(\"The agent successfully kept the pole balanced for some time!\")\n",
        "else:\n",
        "    print(\"The agent struggled to balance the pole.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Q-learning training for Cart-Pole...\n",
            "Episode 500, Total Reward: 200.0, Epsilon: 0.02\n",
            "Episode 1000, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 1500, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 2000, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 2500, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 3000, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 3500, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 4000, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 4500, Total Reward: 200.0, Epsilon: 0.01\n",
            "Episode 5000, Total Reward: 200.0, Epsilon: 0.01\n",
            "\n",
            "--- Cart-Pole Q-Learning Training Complete ---\n",
            "\n",
            "Testing the trained agent for one episode...\n",
            "Test Episode finished in 400 steps with total reward: 400.0\n",
            "The agent successfully kept the pole balanced for some time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 4: Representing Data in Python"
      ],
      "metadata": {
        "id": "NLbgLMW-0Cgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code looked at the raw, unlabeled data and automatically found a way to structure it into groups. Real clustering algorithms use more complex math to do this, but the core idea of finding the underlying structure is the same."
      ],
      "metadata": {
        "id": "Stfl09XKso-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing Data in Python\n",
        "Before we can feed data to a model, we need to represent it in our code. A common way to start is with a list of lists, where each inner list represents a single data point (like one student).\n"
      ],
      "metadata": {
        "id": "wUrC6YIHz1Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This dataset represents three students.\n",
        "# The features are: [Hours Studied, Hours of Sleep]\n",
        "# The label is the last element: \"pass\" or \"fail\"\n",
        "\n",
        "student_data = [\n",
        "    [8, 7, \"pass\"],  # Student 1: 8 hrs study, 7 hrs sleep, passed.\n",
        "    [4, 5, \"fail\"],  # Student 2: 4 hrs study, 5 hrs sleep, failed.\n",
        "    [7, 8, \"pass\"]   # Student 3: 7 hrs study, 8 hrs sleep, passed.\n",
        "]\n",
        "\n",
        "# We can access the data for the first student (at index 0).\n",
        "first_student = student_data[0]\n",
        "\n",
        "# Now we can separate the features from the label for that student.\n",
        "# In Python, slicing with [:-1] means \"get everything except the last element\".\n",
        "student_features = first_student[:-1]\n",
        "\n",
        "# Slicing with [-1] means \"get the very last element\".\n",
        "student_label = first_student[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "qar1WX1cz1s-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data for first student: {first_student}\")\n",
        "print(f\"Features: {student_features}\")\n",
        "print(f\"Label: {student_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tBOwhCNz3u9",
        "outputId": "02b5f6d0-6baa-4a61-97cf-6cf9ea8e6500"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for first student: [8, 7, 'pass']\n",
            "Features: [8, 7]\n",
            "Label: pass\n"
          ]
        }
      ]
    }
  ]
}